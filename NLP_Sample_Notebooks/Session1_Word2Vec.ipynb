{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Session1_Word2Vec.ipynb","provenance":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lgbpOhN2NNmy"},"source":["# Word2Vec Example\n","\n","### Getting and preprocessing the dataset \n","\n","A dataset composed by movie plots (in txt format) is used in this example."]},{"cell_type":"code","metadata":{"id":"BtepfGFQ4R-k","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1618321116953,"user_tz":-120,"elapsed":2454,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"3f6ba670-18a3-48ff-ae50-d8b72648dd4c"},"source":["import gdown\n","\n","url = 'https://drive.google.com/uc?id=1nrHLegM4ee7RoVNZEXMpxskQAMaNu06W'\n","output = \"movie_plots.txt\"\n","gdown.download(url, output, quiet=False)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1nrHLegM4ee7RoVNZEXMpxskQAMaNu06W\n","To: /content/movie_plots.txt\n","20.7MB [00:00, 151MB/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'movie_plots.txt'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"Jk7H52tuhb1Q"},"source":["Once the dataset is downloaded, we need to do some cleaning and preprocessing **before** generating the embeddings. Preprocessing includes tasks such as tokenization, lowercasing and stopword (and empty word) removal. **Gensim** includes a simple preprocessing method that performs these tasks automatically"]},{"cell_type":"code","metadata":{"id":"IUq5VECedO01","executionInfo":{"status":"ok","timestamp":1618321123963,"user_tz":-120,"elapsed":4701,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}}},"source":["import gensim\n","\n","def process_input_file(input_file):\n","  token_list=[]\n","  with open(input_file,'r',encoding='utf-8',errors='ignore') as f:\n","    for line in f:\n","      token_list.append(gensim.utils.simple_preprocess(line))\n","  return token_list\n","\n","documents=process_input_file(\"./movie_plots.txt\")\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJgNRs8HmWka"},"source":["Once this preprocessing is done, we end up with a flat list of meaningful tokens, following the same appearance order as in the input documents. This ensures that word context is maintained. To ensure that this constraint is met, we take a look at the first sentence. "]},{"cell_type":"code","metadata":{"id":"wwonXgyXmmA2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4765319b-ff09-494b-d208-dcbef6cb3725"},"source":["documents[1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['bartender',\n"," 'is',\n"," 'working',\n"," 'at',\n"," 'saloon',\n"," 'serving',\n"," 'drinks',\n"," 'to',\n"," 'customers',\n"," 'after',\n"," 'he',\n"," 'fills',\n"," 'stereotypically',\n"," 'irish',\n"," 'man',\n"," 'bucket',\n"," 'with',\n"," 'beer',\n"," 'carrie',\n"," 'nation',\n"," 'and',\n"," 'her',\n"," 'followers',\n"," 'burst',\n"," 'inside',\n"," 'they',\n"," 'assault',\n"," 'the',\n"," 'irish',\n"," 'man',\n"," 'pulling',\n"," 'his',\n"," 'hat',\n"," 'over',\n"," 'his',\n"," 'eyes',\n"," 'and',\n"," 'then',\n"," 'dumping',\n"," 'the',\n"," 'beer',\n"," 'over',\n"," 'his',\n"," 'head',\n"," 'the',\n"," 'group',\n"," 'then',\n"," 'begin',\n"," 'wrecking',\n"," 'the',\n"," 'bar',\n"," 'smashing',\n"," 'the',\n"," 'fixtures',\n"," 'mirrors',\n"," 'and',\n"," 'breaking',\n"," 'the',\n"," 'cash',\n"," 'register',\n"," 'the',\n"," 'bartender',\n"," 'then',\n"," 'sprays',\n"," 'seltzer',\n"," 'water',\n"," 'in',\n"," 'nation',\n"," 'face',\n"," 'before',\n"," 'group',\n"," 'of',\n"," 'policemen',\n"," 'appear',\n"," 'and',\n"," 'order',\n"," 'everybody',\n"," 'to',\n"," 'leave']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"3ijf2MgGmgXG"},"source":["As shown, all tokens have been converted to lowercase, and some stopwords and empty words have been removed. Still, the context of the original sentence remains."]},{"cell_type":"markdown","metadata":{"id":"hOGWBq5ocBQy"},"source":["### Initializing and training the model"]},{"cell_type":"markdown","metadata":{"id":"WQ2OR_Wlnq2M"},"source":["Once we have our document tokenized, we can initialize and train our Word2Vec model. Prior to training the model, certain parameters need to be defined:\n","\n","\n","*   *size*: Dimensionality of word vectors. It should be consistent with the dimensionality of the document corpus and the size of the vocabulary. **As we have a reduced corpus, a size of 100 should be adecquate for our problem.**\n","*   *window*: Dimension of the context window. It should be enough to contextualize a word. In this example, **we will use a size 10**.\n","*   *min_count*: Minimum number of appearances of a word in the corpus to be considered for embedding. **We will consider that at least 3 repetitions of a word are enough to be embedded**.\n","*   *sg*: Training algorithm: 1 for skip-gram; 0 for CBOW. **We will use skip-gram**.\n","*   *iter*: Number of training iterations. We will do **5 iterations**.\n","*   *seed*: Initialization seed. **We will use 1852 as our seed.**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"JYZ6jJjm3TGJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321361562,"user_tz":-120,"elapsed":233451,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"c55c55ac-c57e-41d5-ac08-dbb7c3f829c9"},"source":["w2v_model=gensim.models.Word2Vec(documents,size=100,window=10,min_count=5,sg=1,iter=5,seed=1852)\n","w2v_model.train(documents, total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n","  \n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["(12958641, 17280470)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"_IIeELcGIxNb"},"source":["After the model has been trained, we can extract the generated word embeddings (in the form of a Python dictionary) and query over them"]},{"cell_type":"code","metadata":{"id":"gjTv53jNRU5e","executionInfo":{"status":"ok","timestamp":1618321374283,"user_tz":-120,"elapsed":559,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}}},"source":["w2v_embeddings=w2v_model.wv"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"piGAcw0RSP3h"},"source":["With the extracted embeddings, we can perform certain operations such as:\n","\n","### Ask for the most similar word to a given word"]},{"cell_type":"code","metadata":{"id":"G6T6TkOZSefR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321376386,"user_tz":-120,"elapsed":557,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"019209af-ce31-42bc-8ac5-de7ad6da848e"},"source":["word=\"singer\"\n","w2v_embeddings.most_similar(positive=word)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('nightclub', 0.7739318609237671),\n"," ('dancer', 0.7558026909828186),\n"," ('hotheaded', 0.7398906946182251),\n"," ('performer', 0.7246049642562866),\n"," ('ingenue', 0.7170679569244385),\n"," ('soprano', 0.7109376788139343),\n"," ('netta', 0.7076431512832642),\n"," ('starlet', 0.706315815448761),\n"," ('superstar', 0.7013533711433411),\n"," ('vocalist', 0.7001423835754395)]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"g49anwHubgEf"},"source":["If we just want to get the *N* most similar words, then:"]},{"cell_type":"code","metadata":{"id":"7gosO3XIbtGG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321380247,"user_tz":-120,"elapsed":770,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"af603b3e-b131-40b5-c576-13db3c4b5e5e"},"source":["word=[\"dog\"]\n","n=3\n","w2v_embeddings.most_similar(positive=word,topn=n)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('labrador', 0.7114201188087463),\n"," ('dogs', 0.7100064158439636),\n"," ('puppy', 0.6919729113578796)]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"UJ5k5M4nqY4G"},"source":["### Check whether a word is represented or not\n","\n","As mentioned before, embeddings are returned in the form of a dictionary of tuples, where words play the role of keys and the embeddings are the values. However, being a special type of dictionary, the vocabulary can't be retrieved using the built-in *dict.keys()* function. The following snippet of code retrieves the vocabulary represented by the W2V model, and then uses list comprehension to detect whether a word is represented or not:"]},{"cell_type":"code","metadata":{"id":"PwtPIR5erSsD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321384325,"user_tz":-120,"elapsed":555,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"43edb9d0-db44-4788-91b8-46be75d72a99"},"source":["w2v_vocabulary=w2v_embeddings.vocab.keys()\n","word='platypus'\n","if word in w2v_vocabulary:\n","  print(True)\n","else:\n","  print(False)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FVKkGmahb7ca"},"source":["### Measure the similarity between pairs of words\n","\n","Given two words existing in the corpus, we can measure the similarity existing between the two. If the model is properly trained, then words that refer to similar concepts, such as synonims, should get high similarity scores."]},{"cell_type":"code","metadata":{"id":"NsVs7j4yccAi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321388155,"user_tz":-120,"elapsed":731,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"cb424651-b05b-4bb3-d0c1-ec05151f47a9"},"source":["word1=\"movie\"\n","word2=\"film\"\n","w2v_embeddings.similarity(word1,word2)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.825347"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"kDjMfkJ8dM3p"},"source":["On the contrary, antonyms should recive low scores"]},{"cell_type":"code","metadata":{"id":"jj_qIcoRdRZZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321390502,"user_tz":-120,"elapsed":515,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"dbc01655-313e-497c-bb3f-8e9ac1c8454f"},"source":["word1=\"great\"\n","word2=\"awful\"\n","w2v_embeddings.similarity(word1,word2)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.32689977"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"QyKXziXkdgZN"},"source":["### Retrieve the embedding of a given word\n","\n","There are two ways of retrieving the vector of a word. The first way is by using the built-in method *get_vector*. This model returns the embedding associated with the input word if it exists in the vocabulary, and error otherwise."]},{"cell_type":"code","metadata":{"id":"6cvVgKFIeCTh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321394927,"user_tz":-120,"elapsed":556,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"ebca908d-4282-44bd-8747-2a1f7f7e2f05"},"source":["word=\"beach\"\n","w2v_embeddings.get_vector(word)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.11691452, -0.1356225 ,  0.10066555,  0.35933575,  0.26329288,\n","       -0.25205   ,  0.24960966, -0.06589814,  0.01037013,  1.0342513 ,\n","       -0.47822237, -0.10310647,  0.29664803, -0.2733945 , -0.25119177,\n","        0.28906366,  0.18302594, -0.37111542,  0.14916371,  0.01552051,\n","        0.11886437, -0.10779689,  0.2465281 ,  0.15453239, -0.3312453 ,\n","        0.13145737, -0.04551599,  0.21664234, -0.3127712 , -0.2797321 ,\n","       -0.2933099 , -0.22166942,  0.18876773, -0.6957975 ,  0.13720626,\n","       -0.10923652,  0.5118466 ,  0.07903919, -0.00449225, -0.50585634,\n","        0.62448114, -0.02667939, -0.19505975,  0.05632718, -0.13266708,\n","        0.17240706,  0.33905092,  0.20434055,  0.3421863 , -0.19651157,\n","        0.11847249, -0.4463276 ,  0.2541473 , -0.0915373 ,  0.31275782,\n","       -0.26218522,  0.10099335, -0.08984791,  0.30922264,  0.56262803,\n","       -0.07055393,  0.35954636,  0.05894015,  0.2949058 ,  0.43041632,\n","       -0.27441806, -0.13565955, -0.00531093, -0.05123455,  0.02115889,\n","       -0.09845719, -0.28885856,  0.00908614,  0.13744959,  0.03175008,\n","        0.7996404 , -0.05422896,  0.17924331,  0.10856087,  0.37107936,\n","        0.125106  , -0.11220908,  0.7136227 , -0.03679804, -0.26905644,\n","        0.76160586,  0.12793224, -0.06988557,  0.04358868, -0.2426635 ,\n","        0.01481202, -0.00747078, -0.23016787, -0.03932096, -0.2337305 ,\n","        0.21722938, -0.21842565, -0.17632139,  0.01798653,  0.18037675],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"7KCP7Mvne2bW"},"source":["Considering that word embeddings are stored in the form of a dictionary composed by *(word,vector)* tuples, where words serve as keys, we can easily obtain the embedding as:"]},{"cell_type":"code","metadata":{"id":"RnK7EvjrfIDg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321398959,"user_tz":-120,"elapsed":644,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"eedc87cf-d9f9-4921-83e2-5905d0ef011c"},"source":["w2v_embeddings['beach']"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.11691452, -0.1356225 ,  0.10066555,  0.35933575,  0.26329288,\n","       -0.25205   ,  0.24960966, -0.06589814,  0.01037013,  1.0342513 ,\n","       -0.47822237, -0.10310647,  0.29664803, -0.2733945 , -0.25119177,\n","        0.28906366,  0.18302594, -0.37111542,  0.14916371,  0.01552051,\n","        0.11886437, -0.10779689,  0.2465281 ,  0.15453239, -0.3312453 ,\n","        0.13145737, -0.04551599,  0.21664234, -0.3127712 , -0.2797321 ,\n","       -0.2933099 , -0.22166942,  0.18876773, -0.6957975 ,  0.13720626,\n","       -0.10923652,  0.5118466 ,  0.07903919, -0.00449225, -0.50585634,\n","        0.62448114, -0.02667939, -0.19505975,  0.05632718, -0.13266708,\n","        0.17240706,  0.33905092,  0.20434055,  0.3421863 , -0.19651157,\n","        0.11847249, -0.4463276 ,  0.2541473 , -0.0915373 ,  0.31275782,\n","       -0.26218522,  0.10099335, -0.08984791,  0.30922264,  0.56262803,\n","       -0.07055393,  0.35954636,  0.05894015,  0.2949058 ,  0.43041632,\n","       -0.27441806, -0.13565955, -0.00531093, -0.05123455,  0.02115889,\n","       -0.09845719, -0.28885856,  0.00908614,  0.13744959,  0.03175008,\n","        0.7996404 , -0.05422896,  0.17924331,  0.10856087,  0.37107936,\n","        0.125106  , -0.11220908,  0.7136227 , -0.03679804, -0.26905644,\n","        0.76160586,  0.12793224, -0.06988557,  0.04358868, -0.2426635 ,\n","        0.01481202, -0.00747078, -0.23016787, -0.03932096, -0.2337305 ,\n","        0.21722938, -0.21842565, -0.17632139,  0.01798653,  0.18037675],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"kkWjGfcbeHUP"},"source":["### Get the closest word to a given vector\n","\n","As words are represented in a vectorial space, we can perform operations certain operations, such as addition or substraction, that give also vectors as a result. As this operations are based on intuition, therefore not exact, the result of an operation like this is not directly keyed to a word. We can get the closest word to a given vector as:\n"]},{"cell_type":"code","metadata":{"id":"vW87ynHjlGcg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321420240,"user_tz":-120,"elapsed":507,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"f5086dbe-f8a5-49ff-a6bb-a96467d76fe0"},"source":["vector1=w2v_embeddings['music']\n","vector2=w2v_embeddings['film']\n","operation=vector1+vector2\n","w2v_embeddings.similar_by_vector(operation)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('music', 0.8312228322029114),\n"," ('film', 0.8091937303543091),\n"," ('comedians', 0.7460590600967407),\n"," ('segue', 0.7459224462509155),\n"," ('movie', 0.7425822615623474),\n"," ('theme', 0.7345827221870422),\n"," ('previn', 0.7257819175720215),\n"," ('storyline', 0.7213714122772217),\n"," ('orchestral', 0.7164971828460693),\n"," ('rhapsody', 0.7151800394058228)]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"gYZTKxYemQae"},"source":["### Detect unfitting terms in a list of words\n","\n","Given a list of words, we can identify which one of them is unrelated to the rest using the built-in function *doesnt_match*:"]},{"cell_type":"code","metadata":{"id":"x-5P5AWpme8V","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1618321427865,"user_tz":-120,"elapsed":965,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"4704aa2c-f892-43eb-fc4b-9dbd72b9a206"},"source":["word_list=['cat','dog','mouse','actress','bird']\n","w2v_embeddings.doesnt_match(word_list)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n","  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'actress'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"F3tprnZ6mukp"},"source":["### Perform analogies between words\n","\n","One of the biggest improvements introduced by Word2Vec was the capability to perform analogies between words. This property not only puts to evidence the quality of the embeddings generated by the model, but enables the **approximate** representation of words that are unseen during training:"]},{"cell_type":"code","metadata":{"id":"ojZGZqEDo0eh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321440495,"user_tz":-120,"elapsed":542,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"a11255e0-54ce-4c70-9476-50aa0965d236"},"source":["word1=w2v_embeddings['cat']\n","word2=w2v_embeddings['cats']\n","word3=w2v_embeddings['mouse']\n","operation=word1-word3+word2\n","w2v_embeddings.similar_by_vector(operation)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('cats', 0.8587690591812134),\n"," ('cat', 0.628413200378418),\n"," ('mice', 0.6051133871078491),\n"," ('scat', 0.5927438735961914),\n"," ('dogs', 0.5771578550338745),\n"," ('lounging', 0.5638996362686157),\n"," ('jaws', 0.5573769211769104),\n"," ('feline', 0.5521344542503357),\n"," ('screeching', 0.5508318543434143),\n"," ('noisy', 0.5451743602752686)]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"VGiE8Ghrp2xB"},"source":["A cleaner and faster way of performing analogical inference is: "]},{"cell_type":"code","metadata":{"id":"X0et0HtRp-Ed","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618321477424,"user_tz":-120,"elapsed":602,"user":{"displayName":"Antonaba Varela","photoUrl":"","userId":"06371348070710609953"}},"outputId":"d35dd59e-a5f3-466c-e6ba-1344c507f066"},"source":["w2v_embeddings.most_similar(positive=['cat', 'cats'], negative=['mouse'])"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('mice', 0.6658698320388794),\n"," ('scat', 0.6330621242523193),\n"," ('jaws', 0.6269769072532654),\n"," ('feline', 0.6197717785835266),\n"," ('dogs', 0.6094540357589722),\n"," ('canine', 0.6084303259849548),\n"," ('canary', 0.6057158708572388),\n"," ('screeching', 0.603484570980072),\n"," ('dog', 0.5941815376281738),\n"," ('hens', 0.5933408141136169)]"]},"metadata":{"tags":[]},"execution_count":18}]}]}